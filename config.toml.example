##########################
##### Probes and Grains
##########################
# For more information about the config file format, visit
# https://github.com/redsift/ingraind/wiki/Configuration
#
# For the list of tags generated by a grain, consult the grain documentation at
# https://github.com/redsift/ingraind/wiki/Configuration

# A list of all directories to monitor.
#
# If the directories are mounts, only the path relative to the _filesystem_ root
# are resolved. Eg. if /var is a different partition, events in /var/lib/docker
# would be returned as /lib/docker
#
[[probe]]
pipelines = ["console"]
[probe.config]
type = "Files"
monitor_dirs = ["/"]

# The TCP4 grain will track outbound TCP/IPv4 connections, as well as
# send/receive metrics about established connections
[[probe]]
pipelines = ["console"]
[probe.config]
type = "TCP4"

# The UDP grain will track send/receive metrics about UDP/IPv4 connections.
[[probe]]
pipelines = ["console"]
[probe.config]
type = "UDP"

# The DNS grain monitors _inbpound_ DNS traffic only.
#
# On a local network, mDNS should also be picked up, as well as all incoming
# answers to outbound DNS queries
#
# A mandatory parameter is `interface`, which needs to specify the interface to
# monitor.
[[probe]]
pipelines = ["console"]
[probe.config]
type = "DNS"
interace = "eth0"

# The TLS grain reports TLS ClientHello and ServerHello packets.

# A mandatory parameter is `interface`, which needs to specify the interface to
# monitor.
[[probe]]
pipelines = ["console"]
[probe.config]
type = "TLS"
interace = "eth0"

##########################
##### Pipeline defintions
##########################
# The console backend dumps every incoming metric to stdout.
# 
# The output is Rust `Debug` objects, which cannot be directly parsed
# by any parser, but is reasonably human readable
[pipeline.console.config]
backend = "Console"

# The HTTP backend sends JSON data to the specified endpoint.
#
# All configuration is done through the config file, including authorization
# tokens, or other credential-style information.
#
# If the server returns with an error, there's no back-off mechanism, or any
# type of awareness of this.
[pipeline.http.config]
backend = "HTTP"
uri = "http://example.redsift.com/insert"
[pipeline.http.config.headers]
authorization = "token"
"custom-header" = "some value"


# The StatsD backend sends incoming metrics to a StatsD server using UDP.
#
# If the server supports Datadog extensions, then `use_tags` can be set to
# `true` to gather extended metadata.
#
# When running the program, `STATSD_HOST` and `STATSD_PORT` environment
# variables need to be set!
[pipeline.statsd.config]
backend = "StatsD"
use_tags = false

# The S3 backend sends incoming metrics to an S3 bucket.
# The files will contain a JSON array, and named like so:
#     hostname_<nanoseconds since UNIX epoch>
#
# It is recommended to use a `Buffer` step in S3 pipelines, to control how often
# a bucket is written.
#
# All configuration is runtime. The following environment variables MUST be set:
#  * AWS_ACCESS_KEY_ID=
#  * AWS_SECRET_ACCESS_KEY=
#  * AWS_S3_BUCKET=
#  * AWS_DEFAULT_REGION=
[pipeline.s3.config]
backend = "S3"

##########################
##### Aggregations/steps
##########################
# Steps will be walked through in order of definition. They may modify the
# content of the data coming from the grains, or omitted altogether.

# The Whitelist filter will only preserve tags with the listed keys for a metric
# record.
[[pipeline.s3.steps]]
type = "Whitelist"
allow = ["k1", "k2"]

# The Regex filter will replace _values_ of tags with `replace_with`, where the
# _values_ match a particular `regex`. Matches are only made for the static
# `key`s described in the pattern entry.
#
# `replace_with` is a constant, and will disregard any regex pattern references.
[[pipeline.s3.steps]]
type = "Regex"
patterns = [
  { key = "some_key", regex = ".*", replace_with = "some_value"},
  { key = "some_key2", regex = ".*", replace_with = "some_value2"},
]

# The Buffer aggregation will gather data for `interval_` seconds, and releases
# it to the next step only after.
# 
# All data points from the same metric with the same set of tags will be
# collapsed into one record, with the `timestamp` indicating the first record's
# `timestamp`.
#
# It makes sense to use the buffer step as last in a pipeline, and strip/filter
# any tags before they start filling up memory.
[[pipeline.statsd.steps]]
type = "Buffer"
interval_s = 30